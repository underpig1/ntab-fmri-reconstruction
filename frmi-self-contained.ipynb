{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 18042,
          "status": "ok",
          "timestamp": 1762118724411,
          "user": {
            "displayName": "Neurotech Berk",
            "userId": "15714532308670007455"
          },
          "user_tz": 480
        },
        "id": "kkcGMw-KZ2fd",
        "outputId": "73c37668-7c7e-487a-940a-fa2aeb8cd47e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'brain-diffuser'...\n",
            "remote: Enumerating objects: 428, done.\u001b[K\n",
            "remote: Counting objects: 100% (428/428), done.\u001b[K\n",
            "remote: Compressing objects: 100% (279/279), done.\u001b[K\n",
            "remote: Total 428 (delta 157), reused 390 (delta 132), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (428/428), 6.15 MiB | 8.85 MiB/s, done.\n",
            "Resolving deltas: 100% (157/157), done.\n",
            "/content/brain-diffuser/data\n",
            "Collecting awscli\n",
            "  Downloading awscli-1.42.64-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting botocore==1.40.64 (from awscli)\n",
            "  Downloading botocore-1.40.64-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting docutils<=0.19,>=0.18.1 (from awscli)\n",
            "  Downloading docutils-0.19-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from awscli)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: PyYAML<6.1,>=3.10 in /usr/local/lib/python3.12/dist-packages (from awscli) (6.0.3)\n",
            "Collecting colorama<0.4.7,>=0.2.5 (from awscli)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting rsa<4.8,>=3.1.2 (from awscli)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.40.64->awscli)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore==1.40.64->awscli) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore==1.40.64->awscli) (2.5.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.40.64->awscli) (1.17.0)\n",
            "Downloading awscli-1.42.64-py3-none-any.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.64-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: rsa, jmespath, docutils, colorama, botocore, s3transfer, awscli\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9.1\n",
            "    Uninstalling rsa-4.9.1:\n",
            "      Successfully uninstalled rsa-4.9.1\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.21.2\n",
            "    Uninstalling docutils-0.21.2:\n",
            "      Successfully uninstalled docutils-0.21.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sphinx 8.2.3 requires docutils<0.22,>=0.20, but you have docutils 0.19 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed awscli-1.42.64 botocore-1.40.64 colorama-0.4.6 docutils-0.19 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ozcelikfu/brain-diffuser.git\n",
        "%cd brain-diffuser/data\n",
        "!pip install awscli\n",
        "# !aws s3 sync s3://natural-scenes-dataset/nsddata ./nsddata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wsfckqNhThF",
        "outputId": "895a1345-4505-4429-c415-4aa476085e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing for NSD download\n",
            "Downloaded 1/3 experiments\n",
            "Downloaded 2/3 experiments\n",
            "Downloaded 3/3 experiments\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 37/37 [02:56<00:00,  4.77s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "print('Preparing for NSD download')\n",
        "os.makedirs('nsddata/experiments/nsd', exist_ok=True)\n",
        "os.makedirs('nsddata_stimuli/stimuli/nsd', exist_ok=True)\n",
        "\n",
        "os.system('aws s3 cp --no-sign-request s3://natural-scenes-dataset/nsddata/experiments/nsd/nsd_expdesign.mat nsddata/experiments/nsd/')\n",
        "print('Downloaded 1/3 experiments')\n",
        "\n",
        "os.system('aws s3 cp --no-sign-request s3://natural-scenes-dataset/nsddata/experiments/nsd/nsd_stim_info_merged.pkl nsddata/experiments/nsd/')\n",
        "print('Downloaded 2/3 experiments')\n",
        "\n",
        "os.system('aws s3 cp --no-sign-request s3://natural-scenes-dataset/nsddata_stimuli/stimuli/nsd/nsd_stimuli.hdf5 nsddata_stimuli/stimuli/nsd/')\n",
        "print('Downloaded 3/3 experiments')\n",
        "\n",
        "for sub in [1]:  #  2,5,7\n",
        "    os.makedirs(f'nsddata_betas/ppdata/subj{sub:02d}/func1pt8mm/betas_fithrf_GLMdenoise_RR', exist_ok=True)\n",
        "    for sess in tqdm(range(1, 38)):\n",
        "        os.system(\n",
        "            f'aws s3 cp --no-sign-request '\n",
        "            f's3://natural-scenes-dataset/nsddata_betas/ppdata/subj{sub:02d}/func1pt8mm/betas_fithrf_GLMdenoise_RR/betas_session{sess:02d}.nii.gz '\n",
        "            f'nsddata_betas/ppdata/subj{sub:02d}/func1pt8mm/betas_fithrf_GLMdenoise_RR/'\n",
        "        )\n",
        "\n",
        "for sub in [1]:\n",
        "    os.makedirs(f'nsddata/ppdata/subj{sub:02d}/func1pt8mm/roi', exist_ok=True)\n",
        "    os.system(\n",
        "        f'aws s3 sync --no-sign-request '\n",
        "        f's3://natural-scenes-dataset/nsddata/ppdata/subj{sub:02d}/func1pt8mm/roi/ '\n",
        "        f'nsddata/ppdata/subj{sub:02d}/func1pt8mm/roi/'\n",
        "    )\n",
        "print('NSD download complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgloM2H1bOYy"
      },
      "outputs": [],
      "source": [
        "print(\"Preparing subj 1 data\")\n",
        "# !python prepare_nsddata.py -sub 1\n",
        "# following code is from prepare_sd_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 209390,
          "status": "ok",
          "timestamp": 1762119577012,
          "user": {
            "displayName": "Neurotech Berk",
            "userId": "15714532308670007455"
          },
          "user_tz": 480
        },
        "id": "2w1RbQoSgD5z",
        "outputId": "04db04b4-2a4a-4238-9434-011098e7a29a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/brain-diffuser/data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2720966901.py:15: DeprecationWarning: Please import `mat_struct` from the `scipy.io.matlab` namespace; the `scipy.io.matlab.mio5_params` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  if isinstance(d[key], spio.matlab.mio5_params.mat_struct):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading fMRI data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27750/27750 [00:00<00:00, 818779.32it/s]\n",
            "100%|██████████| 37/37 [06:31<00:00, 10.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fMRI data loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%cd /content/brain-diffuser/data\n",
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import scipy.io as spio\n",
        "import nibabel as nib\n",
        "from tqdm import tqdm\n",
        "\n",
        "sub = 1\n",
        "assert sub in [1, 2, 5, 7]\n",
        "\n",
        "def loadmat(filename):\n",
        "    def _check_keys(d):\n",
        "        for key in d:\n",
        "            if isinstance(d[key], spio.matlab.mio5_params.mat_struct):\n",
        "                d[key] = _todict(d[key])\n",
        "        return d\n",
        "\n",
        "    def _todict(matobj):\n",
        "        d = {}\n",
        "        for strg in matobj._fieldnames:\n",
        "            elem = matobj.__dict__[strg]\n",
        "            if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
        "                d[strg] = _todict(elem)\n",
        "            elif isinstance(elem, np.ndarray):\n",
        "                d[strg] = _tolist(elem)\n",
        "            else:\n",
        "                d[strg] = elem\n",
        "        return d\n",
        "\n",
        "    def _tolist(ndarray):\n",
        "        elem_list = []\n",
        "        for sub_elem in ndarray:\n",
        "            if isinstance(sub_elem, spio.matlab.mio5_params.mat_struct):\n",
        "                elem_list.append(_todict(sub_elem))\n",
        "            elif isinstance(sub_elem, np.ndarray):\n",
        "                elem_list.append(_tolist(sub_elem))\n",
        "            else:\n",
        "                elem_list.append(sub_elem)\n",
        "        return elem_list\n",
        "\n",
        "    data = spio.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
        "    return _check_keys(data)\n",
        "\n",
        "print('Loading fMRI data')\n",
        "stim_order_f = 'nsddata/experiments/nsd/nsd_expdesign.mat'\n",
        "stim_order = loadmat(stim_order_f)\n",
        "\n",
        "sig_train = {}\n",
        "sig_test = {}\n",
        "num_trials = 37*750\n",
        "for idx in tqdm(range(num_trials)):\n",
        "    nsdId = stim_order['subjectim'][sub-1, stim_order['masterordering'][idx]-1] - 1\n",
        "    if stim_order['masterordering'][idx] > 1000:\n",
        "        sig_train.setdefault(nsdId, []).append(idx)\n",
        "    else:\n",
        "        sig_test.setdefault(nsdId, []).append(idx)\n",
        "\n",
        "train_im_idx = list(sig_train.keys())\n",
        "test_im_idx = list(sig_test.keys())\n",
        "\n",
        "roi_dir = f'nsddata/ppdata/subj{sub:02d}/func1pt8mm/roi/'\n",
        "betas_dir = f'nsddata_betas/ppdata/subj{sub:02d}/func1pt8mm/betas_fithrf_GLMdenoise_RR/'\n",
        "\n",
        "mask_filename = 'nsdgeneral.nii.gz'\n",
        "mask = nib.load(os.path.join(roi_dir, mask_filename)).get_fdata()\n",
        "num_voxel = mask[mask>0].shape[0]\n",
        "\n",
        "fmri = np.zeros((num_trials, num_voxel), dtype=np.float32)\n",
        "for i in tqdm(range(37)):\n",
        "    beta_filename = f\"betas_session{i+1:02d}.nii.gz\"\n",
        "    beta_f = nib.load(os.path.join(betas_dir, beta_filename)).get_fdata().astype(np.float32)\n",
        "    fmri[i*750:(i+1)*750] = beta_f[mask>0].T\n",
        "    del beta_f\n",
        "print(\"fMRI data loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3527,
          "status": "ok",
          "timestamp": 1762119580562,
          "user": {
            "displayName": "Neurotech Berk",
            "userId": "15714532308670007455"
          },
          "user_tz": 480
        },
        "id": "2BZ5i5kroR80",
        "outputId": "dfd86076-04fa-415c-a869-f31afe1c7f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded annots/COCO_73k_annots_curated.npy\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/COCO_73k_annots_curated.npy\"\n",
        "output_path = \"annots/COCO_73k_annots_curated.npy\"\n",
        "\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Check if the request was successful\n",
        "\n",
        "with open(output_path, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(f\"Downloaded {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2t4m1V3i8Fk",
        "outputId": "0ed35df3-bb3a-4fac-9c5f-dd122a16c35d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8859/8859 [00:43<00:00, 204.92it/s]\n"
          ]
        }
      ],
      "source": [
        "f_stim = h5py.File('nsddata_stimuli/stimuli/nsd/nsd_stimuli.hdf5', 'r')\n",
        "stim_ds = f_stim['imgBrick']\n",
        "\n",
        "os.makedirs(f'processed_data/subj{sub:02d}', exist_ok=True)\n",
        "\n",
        "def process_and_save(indices, fmri_dict, prefix):\n",
        "    num_samples = len(indices)\n",
        "    fmri_array = np.zeros((num_samples, num_voxel), dtype=np.float32)\n",
        "    stim_array = np.zeros((num_samples, 425, 425, 3), dtype=np.uint8)\n",
        "\n",
        "    for i, idx in tqdm(enumerate(indices), total=num_samples):\n",
        "        stim_array[i] = stim_ds[idx]\n",
        "        fmri_array[i] = fmri[np.array(sorted(fmri_dict[idx]))].mean(0)\n",
        "        # print(f\"{prefix}: processed {i+1}/{num_samples}\")\n",
        "\n",
        "    np.save(f'processed_data/subj{sub:02d}/{prefix}_fmriavg_nsdgeneral_sub{sub}.npy', fmri_array)\n",
        "    np.save(f'processed_data/subj{sub:02d}/{prefix}_stim_sub{sub}.npy', stim_array)\n",
        "    print(f\"{prefix} data saved.\")\n",
        "\n",
        "process_and_save(train_im_idx, sig_train, \"nsd_train\")\n",
        "process_and_save(test_im_idx, sig_test, \"nsd_test\")\n",
        "\n",
        "annots_cur = np.load('annots/COCO_73k_annots_curated.npy')\n",
        "\n",
        "def save_captions(indices, prefix):\n",
        "    captions_array = np.empty((len(indices), 5), dtype=annots_cur.dtype)\n",
        "    for i, idx in tqdm(enumerate(indices), total=len(indices)):\n",
        "        captions_array[i, :] = annots_cur[idx, :]\n",
        "        # print(f\"{prefix}: processed {i+1}/{len(indices)} captions\")\n",
        "    np.save(f'processed_data/subj{sub:02d}/{prefix}_cap_sub{sub}.npy', captions_array)\n",
        "    print(f\"{prefix} captions saved.\")\n",
        "\n",
        "save_captions(train_im_idx, \"nsd_train\")\n",
        "save_captions(test_im_idx, \"nsd_test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 133691,
          "status": "ok",
          "timestamp": 1762120125031,
          "user": {
            "displayName": "Neurotech Berk",
            "userId": "15714532308670007455"
          },
          "user_tz": 480
        },
        "id": "34zgI4F3VmvQ",
        "outputId": "b4454cc4-daea-4684-dac2-4f5f2329e819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model assets\n",
            "--2025-11-02 21:45:45--  https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-log.jsonl\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.244.1\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.244.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 145115 (142K) [application/octet-stream]\n",
            "Saving to: ‘imagenet64-iter-1600000-log.jsonl’\n",
            "\n",
            "imagenet64-iter-160 100%[===================>] 141.71K   904KB/s    in 0.2s    \n",
            "\n",
            "2025-11-02 21:45:46 (904 KB/s) - ‘imagenet64-iter-1600000-log.jsonl’ saved [145115/145115]\n",
            "\n",
            "--2025-11-02 21:45:46--  https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-model.th\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.244.1\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.244.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 501006513 (478M) [text/plain]\n",
            "Saving to: ‘imagenet64-iter-1600000-model.th’\n",
            "\n",
            "imagenet64-iter-160 100%[===================>] 477.80M  11.6MB/s    in 47s     \n",
            "\n",
            "2025-11-02 21:46:33 (10.2 MB/s) - ‘imagenet64-iter-1600000-model.th’ saved [501006513/501006513]\n",
            "\n",
            "--2025-11-02 21:46:33--  https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-model-ema.th\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.244.1\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.244.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500977841 (478M) [text/plain]\n",
            "Saving to: ‘imagenet64-iter-1600000-model-ema.th’\n",
            "\n",
            "imagenet64-iter-160 100%[===================>] 477.77M  10.1MB/s    in 44s     \n",
            "\n",
            "2025-11-02 21:47:17 (11.0 MB/s) - ‘imagenet64-iter-1600000-model-ema.th’ saved [500977841/500977841]\n",
            "\n",
            "--2025-11-02 21:47:17--  https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-opt.th\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.244.1\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.244.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1001616051 (955M) [text/plain]\n",
            "Saving to: ‘imagenet64-iter-1600000-opt.th’\n",
            "\n",
            "imagenet64-iter-160 100%[===================>] 955.21M  11.7MB/s    in 88s     \n",
            "\n",
            "2025-11-02 21:48:45 (10.8 MB/s) - ‘imagenet64-iter-1600000-opt.th’ saved [1001616051/1001616051]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Downloading model assets\")\n",
        "!wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-log.jsonl\n",
        "!wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-model.th\n",
        "!wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-model-ema.th\n",
        "!wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-opt.th"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 36,
          "status": "ok",
          "timestamp": 1762120125075,
          "user": {
            "displayName": "Neurotech Berk",
            "userId": "15714532308670007455"
          },
          "user_tz": 480
        },
        "id": "IVchx0lHVrRR",
        "outputId": "c5eee836-6edc-4fac-fd35-111ae8b649ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting features\n",
            "/content/brain-diffuser\n"
          ]
        }
      ],
      "source": [
        "print(\"Extracting features\")\n",
        "%cd /content/brain-diffuser\n",
        "!mv /content/brain-diffuser/data//imagenet64-iter-1600000-model-ema.th /content/brain-diffuser/vdvae/model/imagenet64-iter-1600000-model-ema.th\n",
        "# !python scripts/vdvae_extract_features.py -sub 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX87DZddrDkV",
        "outputId": "12a95bd1-417d-4a60-c8e2-7613f8fa9fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Restoring ema vae from vdvae/model/imagenet64-iter-1600000-model-ema.th\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/296 [00:00<?, ?it/s]/tmp/ipython-input-274287667.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n",
            " 27%|██▋       | 81/296 [05:18<12:09,  3.39s/it]"
          ]
        }
      ],
      "source": [
        "%cd /content/brain-diffuser/vdvae\n",
        "\n",
        "import sys, os\n",
        "sys.path.append('/content/brain-diffuser/vdvae')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "# project imports\n",
        "from hps import Hyperparams\n",
        "from utils import maybe_download\n",
        "from vae import VAE\n",
        "from image_utils import *\n",
        "from model_utils import *\n",
        "\n",
        "# ---- Hardcoded parameters ----\n",
        "sub = 1\n",
        "batch_size = 30\n",
        "\n",
        "# ---- Setup H params ----\n",
        "H = {\n",
        "    'image_size': 64,\n",
        "    'image_channels': 3,\n",
        "    'seed': 0,\n",
        "    'port': 29500,\n",
        "    'save_dir': './saved_models/test',\n",
        "    'data_root': './',\n",
        "    'desc': 'test',\n",
        "    'hparam_sets': 'imagenet64',\n",
        "\n",
        "    # IMPORTANT: we are already in /content/brain-diffuser/vdvae\n",
        "    # so the correct paths DO NOT start with \"vdvae/\"\n",
        "    'restore_path': 'imagenet64-iter-1600000-model.th',\n",
        "    'restore_ema_path': 'model/imagenet64-iter-1600000-model-ema.th',\n",
        "    'restore_log_path': 'imagenet64-iter-1600000-log.jsonl',\n",
        "    'restore_optimizer_path': 'imagenet64-iter-1600000-opt.th',\n",
        "\n",
        "    'dataset': 'imagenet64',\n",
        "    'ema_rate': 0.999,\n",
        "    'enc_blocks': '64x11,64d2,32x20,32d2,16x9,16d2,8x8,8d2,4x7,4d4,1x5',\n",
        "    'dec_blocks': '1x2,4m1,4x3,8m4,8x7,16m8,16x15,32m16,32x31,64m32,64x12',\n",
        "    'zdim': 16,\n",
        "    'width': 512,\n",
        "    'custom_width_str': '',\n",
        "    'bottleneck_multiple': 0.25,\n",
        "    'no_bias_above': 64,\n",
        "    'scale_encblock': False,\n",
        "    'test_eval': True,\n",
        "    'warmup_iters': 100,\n",
        "    'num_mixtures': 10,\n",
        "    'grad_clip': 220.0,\n",
        "    'skip_threshold': 380.0,\n",
        "    'lr': 0.00015,\n",
        "    'lr_prior': 0.00015,\n",
        "    'wd': 0.01,\n",
        "    'wd_prior': 0.0,\n",
        "    'num_epochs': 10000,\n",
        "    'n_batch': 4,\n",
        "    'adam_beta1': 0.9,\n",
        "    'adam_beta2': 0.9,\n",
        "    'temperature': 1.0,\n",
        "    'iters_per_ckpt': 25000,\n",
        "    'iters_per_print': 1000,\n",
        "    'iters_per_save': 10000,\n",
        "    'iters_per_images': 10000,\n",
        "    'epochs_per_eval': 1,\n",
        "    'epochs_per_probe': None,\n",
        "    'epochs_per_eval_save': 1,\n",
        "    'num_images_visualize': 8,\n",
        "    'num_variables_visualize': 6,\n",
        "    'num_temperatures_visualize': 3,\n",
        "    'mpi_size': 1,\n",
        "    'local_rank': 0,\n",
        "    'rank': 0,\n",
        "    'logdir': './saved_models/test/log'\n",
        "}\n",
        "\n",
        "class dotdict(dict):\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "H = dotdict(H)\n",
        "\n",
        "# sanity check: make sure EMA checkpoint exists where we say it is\n",
        "if not os.path.exists(H.restore_ema_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Expected EMA checkpoint at {H.restore_ema_path}.\\n\"\n",
        "        \"Make sure you ran wget in /content/brain-diffuser/vdvae and that:\\n\"\n",
        "        \"  imagenet64-iter-1600000-model-ema.th was moved into vdvae/model/.\"\n",
        "    )\n",
        "\n",
        "H, preprocess_fn = set_up_data(H)\n",
        "ema_vae = load_vaes(H)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "ema_vae = ema_vae.to(device)\n",
        "\n",
        "# ---- RAM-safe dataset ----\n",
        "class ExternalImageDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        self.data_path = data_path\n",
        "        self.data = np.load(data_path, mmap_mode='r')  # open once\n",
        "        self.shape = self.data.shape\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx]  # direct access\n",
        "        img = Image.fromarray(img.astype(np.uint8))\n",
        "        img = T.functional.resize(img, (64, 64))\n",
        "        img = torch.tensor(np.array(img)).float()\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.shape[0]\n",
        "\n",
        "# ---- Function to extract latents incrementally and return memmap ----\n",
        "def extract_latents_memmap(dataset, tmp_path, num_latents=31, batch_size=32):\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
        "                        num_workers=min(8, os.cpu_count()), pin_memory=True)\n",
        "\n",
        "    os.makedirs(os.path.dirname(tmp_path), exist_ok=True)\n",
        "\n",
        "    # Estimate latent size\n",
        "    with torch.no_grad():\n",
        "        dummy_x = next(iter(loader))\n",
        "        data_input, _ = preprocess_fn(dummy_x)\n",
        "        activations = ema_vae.encoder.forward(data_input)\n",
        "        _, stats = ema_vae.decoder.forward(activations, get_latents=True)\n",
        "        latent_dim = sum(s['z'].numel() // len(data_input) for s in stats)\n",
        "\n",
        "    # Memory-mapped array\n",
        "    latents_memmap = np.memmap(tmp_path, dtype=np.float32, mode='w+',\n",
        "                               shape=(len(dataset), latent_dim))\n",
        "\n",
        "    idx_start = 0\n",
        "    from torch.cuda.amp import autocast\n",
        "\n",
        "    for i, x in tqdm(enumerate(loader), total=len(loader)):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        with torch.no_grad(), autocast():\n",
        "            data_input, _ = preprocess_fn(x)\n",
        "            activations = ema_vae.encoder.forward(data_input)\n",
        "            _, stats = ema_vae.decoder.forward(activations, get_latents=True)\n",
        "            batch_latent = np.hstack(\n",
        "                [s['z'].cpu().numpy().reshape(len(data_input), -1) for s in stats]\n",
        "            )\n",
        "            latents_memmap[idx_start:idx_start + len(data_input)] = batch_latent\n",
        "            idx_start += len(data_input)\n",
        "\n",
        "    latents_memmap.flush()\n",
        "    return tmp_path\n",
        "\n",
        "# ---- Paths ----\n",
        "# NOTE: we are in /content/brain-diffuser/vdvae\n",
        "# Our NSD data lives in ../data/processed_data/...\n",
        "train_path = f'/content/brain-diffuser/data/processed_data/subj{sub:02d}/nsd_train_stim_sub{sub}.npy'\n",
        "test_path  = f'/content/brain-diffuser/data/processed_data/subj{sub:02d}/nsd_test_stim_sub{sub}.npy'\n",
        "\n",
        "train_dataset = ExternalImageDataset(train_path)\n",
        "test_dataset  = ExternalImageDataset(test_path)\n",
        "\n",
        "# Temporary memmap files\n",
        "train_tmp = f'/content/brain-diffuser/data/extracted_features/subj{sub:02d}/train_latents.dat'\n",
        "test_tmp  = f'/content/brain-diffuser/data/extracted_features/subj{sub:02d}/test_latents.dat'\n",
        "\n",
        "# Extract latents incrementally (RAM-safe)\n",
        "extract_latents_memmap(train_dataset, train_tmp, num_latents=31, batch_size=batch_size)\n",
        "extract_latents_memmap(test_dataset, test_tmp, num_latents=31, batch_size=batch_size)\n",
        "\n",
        "# Load memmap files in read mode and save as a single npz\n",
        "\n",
        "# np.savez_compressed(\n",
        "#     f'/content/brain-diffuser/data/extracted_features/subj{sub:02d}/nsd_vdvae_features_31l.npz',\n",
        "#     train_latents=train_latents,\n",
        "#     test_latents=test_latents\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "sub = 1\n",
        "train_path = '/content/brain-diffuser/data/processed_data/subj{:02d}/nsd_train_fmriavg_nsdgeneral_sub{}.npy'.format(sub,sub)\n",
        "train_fmri = np.load(train_path)\n",
        "test_path = '/content/brain-diffuser/data/processed_data/subj{:02d}/nsd_test_fmriavg_nsdgeneral_sub{}.npy'.format(sub,sub)\n",
        "test_fmri = np.load(test_path)\n",
        "\n",
        "train_tmp = f'/content/brain-diffuser/data/extracted_features/subj{sub:02d}/train_latents.dat'\n",
        "test_tmp  = f'/content/brain-diffuser/data/extracted_features/subj{sub:02d}/test_latents.dat'\n",
        "\n",
        "train_latents = np.memmap(train_tmp, dtype=np.float32, mode='r').reshape(len(train_fmri), -1)\n",
        "test_latents  = np.memmap(test_tmp, dtype=np.float32, mode='r').reshape(len(test_fmri), -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# CONFIG\n",
        "# -------------------------------------------------------\n",
        "MAX_TRAIN = 5000\n",
        "MAX_TEST  = 1000\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 0\n",
        "PIN_MEMORY = False\n",
        "HIDDEN_DIM = 128\n",
        "INPUT_SCALE = 300.0   # BASED ON YOUR ORIGINAL PIPELINE\n",
        "CLIP_GRAD_NORM = 5.0\n",
        "LR = 1e-4             # SMALLER/SAFE BUT LARGER THAN 1e-5 IF YOU WANT FASTER LEARNING\n",
        "EPOCHS = 50\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# RAM-SAFE DATASET (NO HEAP NORMALIZATION, BUT SCALE)\n",
        "# -------------------------------------------------------\n",
        "class FMRI2LatentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, fmri_path, latent_path, fmri_dim, latent_dim, max_samples=None, scale=INPUT_SCALE):\n",
        "        # MEMMAPS ARE 1D ARRAYS OF FLOAT32\n",
        "        self.fmri_raw = np.memmap(fmri_path, dtype=np.float32, mode='r')\n",
        "        self.latent_raw = np.memmap(latent_path, dtype=np.float32, mode='r')\n",
        "\n",
        "        total = len(self.fmri_raw) // fmri_dim\n",
        "        if max_samples is not None:\n",
        "            total = min(total, max_samples)\n",
        "\n",
        "        self.total = total\n",
        "        self.fmri_dim = fmri_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.scale = float(scale)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_f = idx * self.fmri_dim\n",
        "        start_l = idx * self.latent_dim\n",
        "\n",
        "        x = np.array(self.fmri_raw[start_f:start_f + self.fmri_dim], dtype=np.float32)\n",
        "        y = np.array(self.latent_raw[start_l:start_l + self.latent_dim], dtype=np.float32)\n",
        "\n",
        "        # ZERO OUT ANY BAD VALUES\n",
        "        x = np.nan_to_num(x, nan=0., posinf=0., neginf=0.)\n",
        "        y = np.nan_to_num(y, nan=0., posinf=0., neginf=0.)\n",
        "\n",
        "        # SIMPLE SCALE (LIKE YOUR ORIGINAL PIPELINE)\n",
        "        if self.scale != 0:\n",
        "            x = x / self.scale\n",
        "\n",
        "        # CLAMP TO A REASONABLE RANGE TO PREVENT INF/VERY-LARGE VALUES\n",
        "        x = np.clip(x, -1e3, 1e3).astype(np.float32)\n",
        "        y = np.clip(y, -1e6, 1e6).astype(np.float32)\n",
        "\n",
        "        return torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "fmri_dim = train_fmri.shape[1]\n",
        "latent_dim = train_latents.shape[1]\n",
        "\n",
        "train_ds = FMRI2LatentDataset(train_path, train_tmp, fmri_dim, latent_dim, MAX_TRAIN)\n",
        "test_ds  = FMRI2LatentDataset(test_path, test_tmp, fmri_dim, latent_dim, MAX_TEST)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "class FMRI2Latent(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden),\n",
        "            nn.LayerNorm(hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, output_dim),\n",
        "        )\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ENSURE FLOAT32\n",
        "        x = x.float()\n",
        "        # SAFEGUARD: REPLACE NaN/INF WITH 0 BEFORE PASSING FORWARD\n",
        "        x = torch.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        return self.net(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = FMRI2Latent(fmri_dim, latent_dim, hidden=HIDDEN_DIM).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    batches = 0\n",
        "\n",
        "    for xb, yb in tqdm(train_loader, desc=f\"TRAIN EPOCH {epoch+1}\"):\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        # CHECK FOR NaNs/INFs IN INPUTS\n",
        "        if torch.isnan(xb).any() or torch.isinf(xb).any():\n",
        "            print(\"FOUND NaN/INF IN XB! CLEANING (torch.nan_to_num).\")\n",
        "            xb = torch.nan_to_num(xb, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        if torch.isnan(yb).any() or torch.isinf(yb).any():\n",
        "            print(\"FOUND NaN/INF IN YB! CLEANING (torch.nan_to_num).\")\n",
        "            yb = torch.nan_to_num(yb, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "        pred = model(xb)\n",
        "\n",
        "        # CHECK PRED BEFORE LOSS\n",
        "        if torch.isnan(pred).any() or torch.isinf(pred).any():\n",
        "            print(\"NaN/INF IN PRED BEFORE LOSS. ZEROING PRED AND SKIPPING STEP.\")\n",
        "            pred = torch.nan_to_num(pred, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "        loss = criterion(pred, yb)\n",
        "\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(\"LOSS IS NaN/INF -> SKIPPING BACKWARD, printing debug stats\")\n",
        "            print(\"xb mean/std:\", xb.mean().item(), xb.std().item())\n",
        "            print(\"yb mean/std:\", yb.mean().item(), yb.std().item())\n",
        "            print(\"pred mean/std:\", pred.mean().item(), pred.std().item())\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # CHECK GRAD NaNs\n",
        "        any_nan_grad = False\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.grad is not None and (torch.isnan(p.grad).any() or torch.isinf(p.grad).any()):\n",
        "                print(f\"NAN/INF IN GRAD FOR {n}; ZEROING GRADS\")\n",
        "                p.grad = torch.nan_to_num(p.grad, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "                any_nan_grad = True\n",
        "\n",
        "        # OPTIONAL GRAD CLIP\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        batches += 1\n",
        "\n",
        "    if batches:\n",
        "        print(f\"Epoch {epoch+1} - Loss: {total_loss / batches:.6f}\")\n",
        "    else:\n",
        "        print(\"NO BATCHES PROCESSED!\")\n",
        "\n",
        "print(\"TRAINING DONE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMG_TO_PREDICT = 0 # change to which image to predict\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/brain-diffuser/vdvae')\n",
        "import torch\n",
        "import numpy as np\n",
        "#from mpi4py import MPI\n",
        "import socket\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "from hps import Hyperparams, parse_args_and_update_hparams, add_vae_arguments\n",
        "from utils import (logger,\n",
        "                   local_mpi_rank,\n",
        "                   mpi_size,\n",
        "                   maybe_download,\n",
        "                   mpi_rank)\n",
        "from data import mkdir_p\n",
        "from contextlib import contextmanager\n",
        "import torch.distributed as dist\n",
        "#from apex.optimizers import FusedAdam as AdamW\n",
        "from vae import VAE\n",
        "from torch.nn.parallel.distributed import DistributedDataParallel\n",
        "from train_helpers import restore_params\n",
        "from image_utils import *\n",
        "from model_utils import *\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import pickle\n",
        "\n",
        "sub=1\n",
        "assert sub in [1,2,5,7]\n",
        "batch_size=500\n",
        "\n",
        "print('Libs imported')\n",
        "\n",
        "H = {'image_size': 64, 'image_channels': 3,'seed': 0, 'port': 29500, 'save_dir': './saved_models/test', 'data_root': './', 'desc': 'test', 'hparam_sets': 'imagenet64', 'restore_path': 'imagenet64-iter-1600000-model.th', 'restore_ema_path': '/content/brain-diffuser/vdvae/model/imagenet64-iter-1600000-model-ema.th', 'restore_log_path': 'imagenet64-iter-1600000-log.jsonl', 'restore_optimizer_path': 'imagenet64-iter-1600000-opt.th', 'dataset': 'imagenet64', 'ema_rate': 0.999, 'enc_blocks': '64x11,64d2,32x20,32d2,16x9,16d2,8x8,8d2,4x7,4d4,1x5', 'dec_blocks': '1x2,4m1,4x3,8m4,8x7,16m8,16x15,32m16,32x31,64m32,64x12', 'zdim': 16, 'width': 512, 'custom_width_str': '', 'bottleneck_multiple': 0.25, 'no_bias_above': 64, 'scale_encblock': False, 'test_eval': True, 'warmup_iters': 100, 'num_mixtures': 10, 'grad_clip': 220.0, 'skip_threshold': 380.0, 'lr': 0.00015, 'lr_prior': 0.00015, 'wd': 0.01, 'wd_prior': 0.0, 'num_epochs': 10000, 'n_batch': 4, 'adam_beta1': 0.9, 'adam_beta2': 0.9, 'temperature': 1.0, 'iters_per_ckpt': 25000, 'iters_per_print': 1000, 'iters_per_save': 10000, 'iters_per_images': 10000, 'epochs_per_eval': 1, 'epochs_per_probe': None, 'epochs_per_eval_save': 1, 'num_images_visualize': 8, 'num_variables_visualize': 6, 'num_temperatures_visualize': 3, 'mpi_size': 1, 'local_rank': 0, 'rank': 0, 'logdir': './saved_models/test/log'}\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "H = dotdict(H)\n",
        "\n",
        "H, preprocess_fn = set_up_data(H)\n",
        "\n",
        "print('Models is Loading')\n",
        "ema_vae = load_vaes(H)\n",
        "\n",
        "class batch_generator_external_images(Dataset):\n",
        "\n",
        "    def __init__(self, data_path):\n",
        "        self.data_path = data_path\n",
        "        self.im = np.load(data_path).astype(np.uint8)\n",
        "\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        img = Image.fromarray(self.im[idx])\n",
        "        img = T.functional.resize(img,(64,64))\n",
        "        img = torch.tensor(np.array(img)).float()\n",
        "        #img = img/255\n",
        "        #img = img*2 - 1\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return  len(self.im)\n",
        "\n",
        "# GET REF_LATENT FROM FIRST TEST BATCH\n",
        "image_path = '/content/brain-diffuser/data/processed_data/subj{:02d}/nsd_test_stim_sub{}.npy'.format(sub,sub)\n",
        "test_images = batch_generator_external_images(data_path=image_path)\n",
        "testloader = DataLoader(test_images, batch_size=1, shuffle=False)\n",
        "\n",
        "for x in testloader:\n",
        "    data_input, _ = preprocess_fn(x)\n",
        "    with torch.no_grad():\n",
        "        activations = ema_vae.encoder.forward(data_input)\n",
        "        px_z, stats = ema_vae.decoder.forward(activations, get_latents=True)\n",
        "    break  # only need the first batch\n",
        "\n",
        "ref_latent = stats\n",
        "\n",
        "def latent_transformation(latents, ref):\n",
        "    layer_dims = np.array([2**4,2**4,2**8,2**8,2**8,2**8,2**10,2**10,2**10,2**10,2**10,2**10,2**10,2**10,\n",
        "                           2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,\n",
        "                           2**12,2**12,2**14])\n",
        "    transformed_latents = []\n",
        "    for i in range(31):\n",
        "        t_lat = latents[:, layer_dims[:i].sum():layer_dims[:i+1].sum()]\n",
        "        c,h,w = ref[i]['z'].shape[1:]\n",
        "        transformed_latents.append(t_lat.reshape(len(latents), c, h, w))\n",
        "    return transformed_latents\n",
        "\n",
        "# ----------------------\n",
        "# SAMPLE FUNCTION\n",
        "# ----------------------\n",
        "def sample_from_hier_latents(latents, sample_ids):\n",
        "    sample_ids = [id for id in sample_ids if id < len(latents[0])]\n",
        "    layers_num = len(latents)\n",
        "    sample_latents = []\n",
        "    for i in range(layers_num):\n",
        "        sample_latents.append(torch.tensor(latents[i][sample_ids]).float().cuda())\n",
        "    return sample_latents\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "input_latent = latent_transformation(predicted_latent, ref_latent)\n",
        "\n",
        "samp = sample_from_hier_latents(input_latent, [0])  # only one image\n",
        "\n",
        "with torch.no_grad():\n",
        "    px_z = ema_vae.decoder.forward_manual_latents(len(samp[0]), samp, t=None)\n",
        "    sample_from_lat = ema_vae.decoder.out_net.sample(px_z)\n",
        "\n",
        "rec_im = sample_from_lat[0]\n",
        "rec_im = Image.fromarray(rec_im)\n",
        "predicted_img = rec_im\n",
        "rec_im = rec_im.resize((512,512), resample=3)\n",
        "\n",
        "orig_im = test_images[0]\n",
        "if isinstance(orig_im, torch.Tensor):\n",
        "    orig_im = orig_im.numpy().astype(np.uint8)\n",
        "orig_im = Image.fromarray(orig_im)\n",
        "orig_im = orig_im.resize((512,512), resample=3)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(orig_im)\n",
        "plt.title(\"Original\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(rec_im)\n",
        "plt.title(\"Reconstruction\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install huggingface\n",
        "!pip install diffusers\n",
        "!pip install -U peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PHASE 2\n",
        "import torch\n",
        "from diffusers import AutoPipelineForImage2Image, LCMScheduler\n",
        "from diffusers.utils import make_image_grid, load_image\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pipe = AutoPipelineForImage2Image.from_pretrained(\n",
        "    \"Lykon/dreamshaper-7\",\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        ")\n",
        "\n",
        "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
        "\n",
        "generator = torch.manual_seed(0)\n",
        "\n",
        "def generate_image(prompt, init_image):\n",
        "  out_image = pipe(\n",
        "      prompt,\n",
        "      image=init_image,\n",
        "      num_inference_steps=4,\n",
        "      guidance_scale=1,\n",
        "      strength=0.6,\n",
        "      generator=generator\n",
        "  ).images[0]\n",
        "  return out_image\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_caps = np.load(f'/content/brain-diffuser/data/processed_data/subj{sub:02d}/nsd_test_cap_sub{sub}.npy', allow_pickle=True)\n",
        "\n",
        "plt.imshow(predicted_img)\n",
        "plt.axis('off')\n",
        "plt.title(f\"Phase 1 Output Image\")\n",
        "plt.show()\n",
        "start_t = time()\n",
        "label = test_caps[IMG_TO_PREDICT][0]\n",
        "print(label)\n",
        "out = generate_image(label, predicted_img) # THIS IS WHERE TO PUT PROMPT AND IMAGE FROM PHASE 1\n",
        "end_t = time() - start_t\n",
        "plt.imshow(out)\n",
        "plt.axis('off')\n",
        "plt.title(f\"LCM-LoRA Output Image\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
