# ntab-fmri-reconstruction
fMRI Image Reconstruction, Neurotech@Berkeley Fa24-Fa25

Fully self-contained multi-phase image generation:
https://colab.research.google.com/drive/12W2U426YXYYagJ_mpuKH0SZWUtvUUC5w?usp=sharing

Original paper: https://www.nature.com/articles/s41598-023-42891-8

Improvements:
* RAM-safety, massive memory and speedup optimizations to enable accessible Colab computing
* Swapped first phase model with fully-connected MLP regression network that maps fMRI vectors to latent diffusion vectors
* Swapped second phase model with LCM-LoRA latent diffusion prompt-based image generator

This project aimed to reproduce the study presented in the paper "Natural Scene Reconstruction From fMRI Signals Using Generative Latent Diffusion" by Furkan Ozcelik & Rufin VanRullen. The paper focuses on reconstructing visual images directly from human brain activity by analyzing functional magnetic resonance imaging (fMRI) data. We implemented image generation directly from the model, integrating an LCM-LoRA model and PyTorch mini-batch regressors to speed up training from fMRI to latent features. This neural regressor learns the mapping in mini-batches, enabling fast, high-quality images predicting the reverse diffusion process while decreasing compute power and retaining accuracy.

Prior to this work, our team reproduced the two-phase pipeline of Ozcelik et al. (2023), first implementing the complete brain diffuser framework (Phase 1), processing NSD fMRI images, training regression models to map voxel fMRI activity to image representations, and producing CLIP vision/text features. Next, integrating a latent consistency model with LCM-LoRA Luo et al., 2023), fine-tuning and drastically accelerating image generation, replacing the second phase with LoRA (Phase 2) Luo et al., 2023), which generated reconstructions in 2-4 steps instead of the original 50-100 steps. These phases (Phase 1 & Phase 2) were conducted independently, and our work integrates them to generate a model that improves upon the original pipeline introduced by Ozcelik et al. (2023) 

We used the Natural Scenes Dataset (NSD), restricting analyses to subject 1. Preprocessed trial-averaged fMRI responses and their corresponding natural images were loaded through the Brain-Diffuser data pipeline, which provides standardized NumPy train/test splits. All feature arrays were memory-mapped from disk, allowing the full dataset to be accessed without loading it entirely into RAM. To obtain image features, we used the pretrained Very Deep Variational Autoencoder (VDVAE) model trained on ImageNet-64. NSD images were resized to 64×64, passed through the VDVAE encoder–decoder, and latent variables from 31 stochastic layers were extracted and flattened into a single feature vector per image for both training and test sets. Latent tensors were pre-flattened and stored in contiguous memory-mapped blocks to avoid repeated allocation and reduce RAM usage. We then learned a linear mapping from fMRI space to VDVAE latent space. fMRI responses were divided by 300 and z-scored per voxel using statistics from the training set. All fMRI matrices were converted to float32 and memory-mapped, reducing memory load by over half while preserving numerical stability. Using these normalized voxel patterns as inputs and VDVAE latent vectors as targets, we fit an L2-regularized ridge regression model. Ridge regression was performed using a batched, memory-mapped solver that streamed training data in chunks, providing significant speedups and preventing RAM exhaustion. Model performance on held-out test data was assessed using per-latent R² and the mean R² across all latent dimensions. Finally, we reconstructed images directly from the predicted VDVAE latents. Predicted latent vectors were split into 31 segments and reshaped to match the hierarchical latent tensors required by the VDVAE decoder. These latents were fed into the decoder to generate 64×64 reconstructions, which were then upsampled for qualitative comparison to the original NSD stimuli.
